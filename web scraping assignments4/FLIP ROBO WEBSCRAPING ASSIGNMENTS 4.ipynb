{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64efba27",
   "metadata": {},
   "source": [
    "# [FLIP ROBO TECHONOLGIES]\n",
    "\n",
    "# [ASSIGNMENTS4]\n",
    "\n",
    "NAME: BIJAY PAREEK\n",
    "\n",
    "BATCH NO: DS2308\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9237731",
   "metadata": {},
   "source": [
    "1. Scrape the details of most viewed videos on YouTube from Wikipedia. Url\n",
    "= https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos You need to find following details:\n",
    "\n",
    "A)Rank\n",
    "\n",
    "B) Name\n",
    "\n",
    "C) Artist\n",
    "\n",
    "D) Upload date\n",
    "\n",
    "E) Views "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "922e018f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\raj\\anaconda3\\lib\\site-packages (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install beautifulsoup4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "864cbed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\raj\\anaconda3\\lib\\site-packages (2.28.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from requests) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from requests) (2023.7.22)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from requests) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a92f30e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Rank                                             Name  \\\n",
      "0    1.                            \"Baby Shark Dance\"[6]   \n",
      "1    2.                                   \"Despacito\"[9]   \n",
      "2    3.                       \"Johny Johny Yes Papa\"[17]   \n",
      "3    4.                                  \"Bath Song\"[18]   \n",
      "4    5.                               \"Shape of You\"[19]   \n",
      "5    6.                              \"See You Again\"[22]   \n",
      "6    7.                          \"Wheels on the Bus\"[27]   \n",
      "7    8.                \"Phonics Song with Two Words\"[28]   \n",
      "8    9.                                \"Uptown Funk\"[29]   \n",
      "9   10.  \"Learning Colors – Colorful Eggs on a Farm\"[30]   \n",
      "10  11.                              \"Gangnam Style\"[31]   \n",
      "11  12.   \"Masha and the Bear – Recipe for Disaster\"[36]   \n",
      "12  13.                             \"Dame Tu Cosita\"[37]   \n",
      "13  14.                                     \"Axel F\"[38]   \n",
      "14  15.                                      \"Sugar\"[39]   \n",
      "15  16.                             \"Counting Stars\"[40]   \n",
      "16  17.                                       \"Roar\"[41]   \n",
      "17  18.                        \"Baa Baa Black Sheep\"[42]   \n",
      "18  19.           \"Waka Waka (This Time for Africa)\"[43]   \n",
      "19  20.                                      \"Sorry\"[44]   \n",
      "20  21.                             \"Lakdi Ki Kathi\"[45]   \n",
      "21  22.                          \"Thinking Out Loud\"[46]   \n",
      "22  23.                                 \"Dark Horse\"[47]   \n",
      "23  24.          \"Humpty the train on a fruits ride\"[48]   \n",
      "24  25.                                    \"Perfect\"[49]   \n",
      "25  26.                                 \"Let Her Go\"[50]   \n",
      "26  27.                                      \"Faded\"[51]   \n",
      "27  28.                             \"Girls Like You\"[52]   \n",
      "28  29.                      \"Shree Hanuman Chalisa\"[53]   \n",
      "29  30.                                    \"Lean On\"[54]   \n",
      "\n",
      "                                               Artist Upload Date  \\\n",
      "0         Pinkfong Baby Shark - Kids' Songs & Stories       13.48   \n",
      "1                                          Luis Fonsi        8.28   \n",
      "2   LooLoo Kids - Nursery Rhymes and Children's Songs        6.82   \n",
      "3                          Cocomelon - Nursery Rhymes        6.45   \n",
      "4                                          Ed Sheeran        6.11   \n",
      "5                                         Wiz Khalifa        6.05   \n",
      "6                          Cocomelon - Nursery Rhymes        5.62   \n",
      "7               ChuChu TV Nursery Rhymes & Kids Songs        5.52   \n",
      "8                                         Mark Ronson        5.05   \n",
      "9                                         Miroshka TV        4.99   \n",
      "10                                        officialpsy        4.92   \n",
      "11                                         Get Movies        4.56   \n",
      "12                                      Ultra Records        4.46   \n",
      "13                                         Crazy Frog        4.09   \n",
      "14                                           Maroon 5        3.95   \n",
      "15                                        OneRepublic        3.89   \n",
      "16                                         Katy Perry        3.89   \n",
      "17                         Cocomelon - Nursery Rhymes        3.80   \n",
      "18                                            Shakira        3.75   \n",
      "19                                      Justin Bieber        3.72   \n",
      "20                                       Jingle Toons        3.71   \n",
      "21                                         Ed Sheeran        3.67   \n",
      "22                                         Katy Perry        3.60   \n",
      "23      Kiddiestv Hindi - Nursery Rhymes & Kids Songs        3.58   \n",
      "24                                         Ed Sheeran        3.56   \n",
      "25                                          Passenger        3.53   \n",
      "26                                        Alan Walker        3.53   \n",
      "27                                           Maroon 5        3.50   \n",
      "28                              T-Series Bhakti Sagar        3.48   \n",
      "29                               Major Lazer Official        3.48   \n",
      "\n",
      "                Views  \n",
      "0       June 17, 2016  \n",
      "1    January 12, 2017  \n",
      "2     October 8, 2016  \n",
      "3         May 2, 2018  \n",
      "4    January 30, 2017  \n",
      "5       April 6, 2015  \n",
      "6        May 24, 2018  \n",
      "7       March 6, 2014  \n",
      "8   November 19, 2014  \n",
      "9   February 27, 2018  \n",
      "10      July 15, 2012  \n",
      "11   January 31, 2012  \n",
      "12      April 5, 2018  \n",
      "13      June 16, 2009  \n",
      "14   January 14, 2015  \n",
      "15       May 31, 2013  \n",
      "16  September 5, 2013  \n",
      "17      June 25, 2018  \n",
      "18       June 4, 2010  \n",
      "19   October 22, 2015  \n",
      "20      June 14, 2018  \n",
      "21    October 7, 2014  \n",
      "22  February 20, 2014  \n",
      "23   January 26, 2018  \n",
      "24   November 9, 2017  \n",
      "25      July 25, 2012  \n",
      "26   December 3, 2015  \n",
      "27       May 31, 2018  \n",
      "28       May 10, 2011  \n",
      "29     March 22, 2015  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Send an HTTP GET request to the Wikipedia page\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content of the page\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Find the table that contains the data for most viewed YouTube videos\n",
    "table = soup.find(\"table\", {\"class\": \"wikitable\"})\n",
    "\n",
    "# Create empty lists to store the details\n",
    "rank_list = []\n",
    "name_list = []\n",
    "artist_list = []\n",
    "upload_date_list = []\n",
    "views_list = []\n",
    "\n",
    "# Find and scrape the data from the table\n",
    "for row in table.find_all(\"tr\")[1:]:\n",
    "    columns = row.find_all(\"td\")\n",
    "    if len(columns) >= 5:\n",
    "        rank = columns[0].text.strip()\n",
    "        name = columns[1].text.strip()\n",
    "        artist = columns[2].text.strip()\n",
    "        upload_date = columns[3].text.strip()\n",
    "        views = columns[4].text.strip()\n",
    "\n",
    "        rank_list.append(rank)\n",
    "        name_list.append(name)\n",
    "        artist_list.append(artist)\n",
    "        upload_date_list.append(upload_date)\n",
    "        views_list.append(views)\n",
    "\n",
    "# Create a Pandas DataFrame from the scraped data\n",
    "data = {\n",
    "    \"Rank\": rank_list,\n",
    "    \"Name\": name_list,\n",
    "    \"Artist\": artist_list,\n",
    "    \"Upload Date\": upload_date_list,\n",
    "    \"Views\": views_list\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Print or manipulate the DataFrame as needed\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b87c44",
   "metadata": {},
   "source": [
    "2. Scrape the details team India’s international fixtures from bcci.tv.\n",
    "Url = https://www.bcci.tv/.\n",
    "You need to find following details:\n",
    "\n",
    "A) Series\n",
    "\n",
    "B) Place\n",
    "\n",
    "C) Date\n",
    "\n",
    "D) Time\n",
    "\n",
    "Note: - From bcci.tv home page you have reach to the international fixture page through code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57308761",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "from selenium.common.exceptions import ElementNotInteractableException\n",
    "import time\n",
    "from selenium.webdriver.common.by import By\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d200355e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# activating the browser\n",
    "driver = webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "244195bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the search URL\n",
    "driver.get(\"https://www.bcci.tv/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ddafa866",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_button=driver.find_element(By.XPATH,'//button[@class=\"close-button page-close\"]')\n",
    "cross_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58941b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "international_fixture=driver.find_element(By.XPATH,'//*[@id=\"pills-international-tab\"]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60949896",
   "metadata": {},
   "outputs": [],
   "source": [
    "international_fixture.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7dddbae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(100):\n",
    "    driver.execute_script(\"window.scrollBy(0,200)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "95fb5742",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing list empty\n",
    "Series=[]\n",
    "Place=[]\n",
    "Date=[]\n",
    "Time=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "892045d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting content from website\n",
    "Series_tags = driver.find_elements(By.XPATH,'//h5[@class=\"match-tournament-name ng-binding\"]')\n",
    "for i in Series_tags[0:20]:\n",
    "    series=i.text\n",
    "    Series.append(series)\n",
    "\n",
    "\n",
    "Place_tags = driver.find_elements(By.XPATH,'//*[@id=\"match-card\"]/div[2]/div[1]/span[2]')\n",
    "for i in Place_tags[0:20]:\n",
    "    place=i.text\n",
    "    Place.append(place)\n",
    "    \n",
    "\n",
    "Date_tags = driver.find_elements(By.XPATH,'//div[@class=\"match-dates ng-binding\"]')  \n",
    "for i in Date_tags[0:20]:\n",
    "    date=i.text\n",
    "    Date.append(date)   \n",
    "    \n",
    "    \n",
    "Time_tags = driver.find_elements(By.XPATH,'//div[@class=\"match-time no-margin ng-binding\"]')  \n",
    "for i in Time_tags[0:20]:\n",
    "    time_u=i.text\n",
    "    Time.append(time_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a0d7051",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tournament</th>\n",
       "      <th>Venue</th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ICC MENS WORLD CUP 2023</td>\n",
       "      <td>Dharamsala</td>\n",
       "      <td>22 OCTOBER, 2023</td>\n",
       "      <td>2:00 PM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ICC MENS WORLD CUP 2023</td>\n",
       "      <td>Lucknow</td>\n",
       "      <td>29 OCTOBER, 2023</td>\n",
       "      <td>2:00 PM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ICC MENS WORLD CUP 2023</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>2 NOVEMBER, 2023</td>\n",
       "      <td>2:00 PM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ICC MENS WORLD CUP 2023</td>\n",
       "      <td>Kolkata</td>\n",
       "      <td>5 NOVEMBER, 2023</td>\n",
       "      <td>2:00 PM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ICC MENS WORLD CUP 2023</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>12 NOVEMBER, 2023</td>\n",
       "      <td>2:00 PM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>QUADRANGULAR MENS U19 ONE DAY SERIES</td>\n",
       "      <td>Vijayawada</td>\n",
       "      <td>13 NOVEMBER, 2023</td>\n",
       "      <td>9:00 AM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>QUADRANGULAR MENS U19 ONE DAY SERIES</td>\n",
       "      <td>Vijayawada</td>\n",
       "      <td>13 NOVEMBER, 2023</td>\n",
       "      <td>9:00 AM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>QUADRANGULAR MENS U19 ONE DAY SERIES</td>\n",
       "      <td>Vijayawada</td>\n",
       "      <td>15 NOVEMBER, 2023</td>\n",
       "      <td>9:00 AM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>QUADRANGULAR MENS U19 ONE DAY SERIES</td>\n",
       "      <td>Vijayawada</td>\n",
       "      <td>15 NOVEMBER, 2023</td>\n",
       "      <td>9:00 AM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>QUADRANGULAR MENS U19 ONE DAY SERIES</td>\n",
       "      <td>Vijayawada</td>\n",
       "      <td>17 NOVEMBER, 2023</td>\n",
       "      <td>9:00 AM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>QUADRANGULAR MENS U19 ONE DAY SERIES</td>\n",
       "      <td>Vijayawada</td>\n",
       "      <td>17 NOVEMBER, 2023</td>\n",
       "      <td>9:00 AM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>QUADRANGULAR MENS U19 ONE DAY SERIES</td>\n",
       "      <td>Vijayawada</td>\n",
       "      <td>20 NOVEMBER, 2023</td>\n",
       "      <td>9:00 AM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>QUADRANGULAR MENS U19 ONE DAY SERIES</td>\n",
       "      <td>Vijayawada</td>\n",
       "      <td>20 NOVEMBER, 2023</td>\n",
       "      <td>9:00 AM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>QUADRANGULAR MENS U19 ONE DAY SERIES</td>\n",
       "      <td>Vijayawada</td>\n",
       "      <td>22 NOVEMBER, 2023</td>\n",
       "      <td>9:00 AM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>QUADRANGULAR MENS U19 ONE DAY SERIES</td>\n",
       "      <td>Vijayawada</td>\n",
       "      <td>22 NOVEMBER, 2023</td>\n",
       "      <td>9:00 AM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>AUSTRALIA TOUR OF INDIA 2023-24</td>\n",
       "      <td>Visakhapatnam</td>\n",
       "      <td>23 NOVEMBER, 2023</td>\n",
       "      <td>7:00 PM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>QUADRANGULAR MENS U19 ONE DAY SERIES</td>\n",
       "      <td>Vijayawada</td>\n",
       "      <td>24 NOVEMBER, 2023</td>\n",
       "      <td>9:00 AM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>QUADRANGULAR MENS U19 ONE DAY SERIES</td>\n",
       "      <td>Vijayawada</td>\n",
       "      <td>24 NOVEMBER, 2023</td>\n",
       "      <td>9:00 AM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>AUSTRALIA TOUR OF INDIA 2023-24</td>\n",
       "      <td>Thiruvananthapuram</td>\n",
       "      <td>26 NOVEMBER, 2023</td>\n",
       "      <td>7:00 PM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>QUADRANGULAR MENS U19 ONE DAY SERIES</td>\n",
       "      <td>Vijayawada</td>\n",
       "      <td>27 NOVEMBER, 2023</td>\n",
       "      <td>9:00 AM IST</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Tournament               Venue  \\\n",
       "0                ICC MENS WORLD CUP 2023          Dharamsala   \n",
       "1                ICC MENS WORLD CUP 2023             Lucknow   \n",
       "2                ICC MENS WORLD CUP 2023              Mumbai   \n",
       "3                ICC MENS WORLD CUP 2023             Kolkata   \n",
       "4                ICC MENS WORLD CUP 2023           Bengaluru   \n",
       "5   QUADRANGULAR MENS U19 ONE DAY SERIES          Vijayawada   \n",
       "6   QUADRANGULAR MENS U19 ONE DAY SERIES          Vijayawada   \n",
       "7   QUADRANGULAR MENS U19 ONE DAY SERIES          Vijayawada   \n",
       "8   QUADRANGULAR MENS U19 ONE DAY SERIES          Vijayawada   \n",
       "9   QUADRANGULAR MENS U19 ONE DAY SERIES          Vijayawada   \n",
       "10  QUADRANGULAR MENS U19 ONE DAY SERIES          Vijayawada   \n",
       "11  QUADRANGULAR MENS U19 ONE DAY SERIES          Vijayawada   \n",
       "12  QUADRANGULAR MENS U19 ONE DAY SERIES          Vijayawada   \n",
       "13  QUADRANGULAR MENS U19 ONE DAY SERIES          Vijayawada   \n",
       "14  QUADRANGULAR MENS U19 ONE DAY SERIES          Vijayawada   \n",
       "15       AUSTRALIA TOUR OF INDIA 2023-24       Visakhapatnam   \n",
       "16  QUADRANGULAR MENS U19 ONE DAY SERIES          Vijayawada   \n",
       "17  QUADRANGULAR MENS U19 ONE DAY SERIES          Vijayawada   \n",
       "18       AUSTRALIA TOUR OF INDIA 2023-24  Thiruvananthapuram   \n",
       "19  QUADRANGULAR MENS U19 ONE DAY SERIES          Vijayawada   \n",
       "\n",
       "                 Date         Time  \n",
       "0    22 OCTOBER, 2023  2:00 PM IST  \n",
       "1    29 OCTOBER, 2023  2:00 PM IST  \n",
       "2    2 NOVEMBER, 2023  2:00 PM IST  \n",
       "3    5 NOVEMBER, 2023  2:00 PM IST  \n",
       "4   12 NOVEMBER, 2023  2:00 PM IST  \n",
       "5   13 NOVEMBER, 2023  9:00 AM IST  \n",
       "6   13 NOVEMBER, 2023  9:00 AM IST  \n",
       "7   15 NOVEMBER, 2023  9:00 AM IST  \n",
       "8   15 NOVEMBER, 2023  9:00 AM IST  \n",
       "9   17 NOVEMBER, 2023  9:00 AM IST  \n",
       "10  17 NOVEMBER, 2023  9:00 AM IST  \n",
       "11  20 NOVEMBER, 2023  9:00 AM IST  \n",
       "12  20 NOVEMBER, 2023  9:00 AM IST  \n",
       "13  22 NOVEMBER, 2023  9:00 AM IST  \n",
       "14  22 NOVEMBER, 2023  9:00 AM IST  \n",
       "15  23 NOVEMBER, 2023  7:00 PM IST  \n",
       "16  24 NOVEMBER, 2023  9:00 AM IST  \n",
       "17  24 NOVEMBER, 2023  9:00 AM IST  \n",
       "18  26 NOVEMBER, 2023  7:00 PM IST  \n",
       "19  27 NOVEMBER, 2023  9:00 AM IST  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataframe for scrapped data\n",
    "Cricket=pd.DataFrame({'Tournament':Series,'Venue':Place,'Date':Date,'Time':Time})\n",
    "Cricket"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8f3682",
   "metadata": {},
   "source": [
    "3. Scrape the details of State-wise GDP of India from statisticstime.com.\n",
    "Url = http://statisticstimes.com/\n",
    "\n",
    "You have to find following details:\n",
    "\n",
    "A) Rank\n",
    "\n",
    "B) State\n",
    "\n",
    "C) GSDP(18-19)- at current prices\n",
    "\n",
    "D) GSDP(19-20)- at current prices\n",
    "\n",
    "E) Share(18-19)\n",
    "\n",
    "F) GDP($ billion)\n",
    "\n",
    "Note: - From statisticstimes home page you have to reach to economy page through code. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3749798d",
   "metadata": {},
   "source": [
    "4. Scrape the details of trending repositories on Github.com.\n",
    "Url = https://github.com/\n",
    "You have to find the following details:\n",
    "    \n",
    "A) Repository title\n",
    "\n",
    "B) Repository description\n",
    "\n",
    "C) Contributors count\n",
    "\n",
    "D) Language used \n",
    "\n",
    "Note: - From the home page you have to click on the trending option from Explore menu through code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fd306d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "from selenium.common.exceptions import ElementNotInteractableException\n",
    "import time\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "71cb16b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6e380cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening github webpage\n",
    "url='https://github.com/'\n",
    "driver.get(url)\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "51b8229e",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_down=driver.find_element(By.XPATH,'/html/body/div[1]/div[1]/header/div/div[2]/div/nav/ul/li[3]')\n",
    "drop_down.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "da156f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "open_source=driver.find_element(By.XPATH,'/html/body/div[1]/div[1]/header/div/div[2]/div/nav/ul/li[3]/button')\n",
    "open_source.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "61dc2d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clicking on trending option\n",
    "trending=driver.find_element(By.XPATH,'/html/body/div[1]/div[1]/header/div/div[2]/div/nav/ul/li[3]/div/div[3]/ul/li[2]/a')\n",
    "try:\n",
    "    trending.click()\n",
    "except ElementNotInteractableException:\n",
    "    driver.get(trending.get_attribute('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ea217131",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize empty list\n",
    "Title=[]\n",
    "Description=[]\n",
    "Count=[]\n",
    "Language=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eabc4607",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting the data\n",
    "Title_tags=driver.find_elements(By.XPATH,'//span[@class=\"text-normal\"]')\n",
    "for i in  Title_tags:\n",
    "    title=i.text\n",
    "    Title.append(title)\n",
    "Description_tags=driver.find_elements(By.XPATH,'//p[@class=\"col-9 color-fg-muted my-1 pr-4\"]')\n",
    "for i in  Description_tags:\n",
    "    desctiption=i.text\n",
    "    Description.append(desctiption)\n",
    "Count_tags=driver.find_elements(By.XPATH,'//a[@class=\"Link Link--muted d-inline-block mr-3\"]')\n",
    "for i in  Count_tags[1::2]:\n",
    "    count=i.text\n",
    "    Count.append(count)\n",
    "Language_tags=driver.find_elements(By.XPATH,'//span[@class=\"d-inline-block ml-0 mr-3\"]')\n",
    "for i in  Language_tags:\n",
    "    language=i.text\n",
    "    Language.append(language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "11cef666",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Description</th>\n",
       "      <th>Contributors_count</th>\n",
       "      <th>language_used</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Title, Description, Contributors_count, language_used]\n",
       "Index: []"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating dataframe\n",
    "Github=pd.DataFrame({'Title':Title[0:20],'Description':Description[0:20],\n",
    "                'Contributors_count':Count[0:20],'language_used':Language[0:20]})\n",
    "\n",
    "Github"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a57ea1",
   "metadata": {},
   "source": [
    "5. Scrape the details of top 100 songs on billiboard.com. Url = https:/www.billboard.com/ You have to find the\n",
    "following details:\n",
    "    \n",
    "A) Song name\n",
    "\n",
    "B) Artist name\n",
    "\n",
    "C) Last week rank\n",
    "\n",
    "D) Peak rank\n",
    "\n",
    "E) Weeks on board\n",
    "\n",
    " Note: - From the home page you have to click on the charts option then hot 100-page link through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e8303a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\raj\\anaconda3\\lib\\site-packages (2.28.1)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\raj\\anaconda3\\lib\\site-packages (4.11.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from requests) (2023.7.22)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from requests) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from requests) (1.26.11)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.3.1)\n"
     ]
    }
   ],
   "source": [
    "pip install requests beautifulsoup4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8c4d9297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Song Name, Artist Name, Last Week Rank, Peak Rank, Weeks on Board]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Send a GET request to the Billboard Hot 100 page\n",
    "url = \"https://www.billboard.com/charts/hot-100\"\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Find the list of songs\n",
    "    song_list = soup.find_all(\"span\", class_=\"chart-element__information\")\n",
    "\n",
    "    # Initialize lists to store data\n",
    "    song_names = []\n",
    "    artist_names = []\n",
    "    last_week_ranks = []\n",
    "    peak_ranks = []\n",
    "    weeks_on_board = []\n",
    "\n",
    "    # Extract data for each song\n",
    "    for song in song_list:\n",
    "        song_name = song.find(\"span\", class_=\"chart-element__information__song\").get_text()\n",
    "        artist_name = song.find(\"span\", class_=\"chart-element__information__artist\").get_text()\n",
    "        last_week_rank = song.find(\"span\", class_=\"chart-element__information__delta__text\").get_text()\n",
    "        peak_rank = song.find(\"span\", class_=\"chart-element__information__delta__text\").find_next(\"span\").get_text()\n",
    "        weeks_on_chart = song.find(\"span\", class_=\"chart-element__information__delta__text\").find_next(\"span\").find_next(\"span\").get_text()\n",
    "\n",
    "        song_names.append(song_name)\n",
    "        artist_names.append(artist_name)\n",
    "        last_week_ranks.append(last_week_rank)\n",
    "        peak_ranks.append(peak_rank)\n",
    "        weeks_on_board.append(weeks_on_chart)\n",
    "\n",
    "    # Create a DataFrame from the collected data\n",
    "    data = {\n",
    "        'Song Name': song_names,\n",
    "        'Artist Name': artist_names,\n",
    "        'Last Week Rank': last_week_ranks,\n",
    "        'Peak Rank': peak_ranks,\n",
    "        'Weeks on Board': weeks_on_board\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Print the DataFrame\n",
    "    print(df)\n",
    "\n",
    "else:\n",
    "    print(\"Failed to retrieve the webpage. Status code:\", response.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346f0554",
   "metadata": {},
   "source": [
    "6. Scrape the details of Highest selling novels.\n",
    "\n",
    "A) Book name\n",
    "\n",
    "B) Author name\n",
    "\n",
    "C) Volumes sold\n",
    "\n",
    "D) Publisher\n",
    "\n",
    "E) Genre\n",
    "\n",
    " Url - https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0793f8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "from selenium.common.exceptions import ElementNotInteractableException\n",
    "import time\n",
    "\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8a35f0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting to webdriver\n",
    "driver=webdriver.Chrome()\n",
    "time.sleep(1)\n",
    "\n",
    "# Opening Wikipedia webpage\n",
    "url='https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare'\n",
    "driver.get(url)\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3d27ed6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Empty Lists\n",
    "Book =[]\n",
    "Author =[]\n",
    "Volumes_sold =[]\n",
    "Publisher =[]\n",
    "Genre =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1d863a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping Book name\n",
    "try:\n",
    "    book=driver.find_elements(By.XPATH,'//table[@class=\"in-article sortable\"]/tbody/tr/td[2]')\n",
    "    for i in book:\n",
    "        Book.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Book.append('NA')\n",
    "    \n",
    "# Scraping Book author's via Xpath\n",
    "try:\n",
    "    author=driver.find_elements(By.XPATH,'//table[@class=\"in-article sortable\"]/tbody/tr/td[3]')\n",
    "    for i in author:\n",
    "        Author.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Author.append('NA')\n",
    "    \n",
    "# Scraping Volumes sold via Xpath\n",
    "try:\n",
    "    sold=driver.find_elements(By.XPATH,'//table[@class=\"in-article sortable\"]/tbody/tr/td[4]')\n",
    "    for i in sold:\n",
    "        Volumes_sold.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Volumes_sold.append('NA')\n",
    "    \n",
    "# Scraping publisher via xPath\n",
    "try:\n",
    "    publisher=driver.find_elements(By.XPATH,'//table[@class=\"in-article sortable\"]/tbody/tr/td[5]')\n",
    "    for i in publisher:\n",
    "        Publisher.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Publisher.append('NA')\n",
    "    \n",
    "# Scraping Genre via Xpath\n",
    "try:\n",
    "    genre=driver.find_elements(By.XPATH,'//table[@class=\"in-article sortable\"]/tbody/tr/td[6]')\n",
    "    for i in genre:\n",
    "        Genre.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Genre.append('NA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "75260261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mBest Selling Books of All Time List by The Guardian  :\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Book Title</th>\n",
       "      <th>Author Name</th>\n",
       "      <th>Volumes sold</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>Genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Da Vinci Code,The</td>\n",
       "      <td>Brown, Dan</td>\n",
       "      <td>5,094,805</td>\n",
       "      <td>Transworld</td>\n",
       "      <td>Crime, Thriller &amp; Adventure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Harry Potter and the Deathly Hallows</td>\n",
       "      <td>Rowling, J.K.</td>\n",
       "      <td>4,475,152</td>\n",
       "      <td>Bloomsbury</td>\n",
       "      <td>Children's Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Harry Potter and the Philosopher's Stone</td>\n",
       "      <td>Rowling, J.K.</td>\n",
       "      <td>4,200,654</td>\n",
       "      <td>Bloomsbury</td>\n",
       "      <td>Children's Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Harry Potter and the Order of the Phoenix</td>\n",
       "      <td>Rowling, J.K.</td>\n",
       "      <td>4,179,479</td>\n",
       "      <td>Bloomsbury</td>\n",
       "      <td>Children's Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fifty Shades of Grey</td>\n",
       "      <td>James, E. L.</td>\n",
       "      <td>3,758,936</td>\n",
       "      <td>Random House</td>\n",
       "      <td>Romance &amp; Sagas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Ghost,The</td>\n",
       "      <td>Harris, Robert</td>\n",
       "      <td>807,311</td>\n",
       "      <td>Random House</td>\n",
       "      <td>General &amp; Literary Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Happy Days with the Naked Chef</td>\n",
       "      <td>Oliver, Jamie</td>\n",
       "      <td>794,201</td>\n",
       "      <td>Penguin</td>\n",
       "      <td>Food &amp; Drink: General</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Hunger Games,The:Hunger Games Trilogy</td>\n",
       "      <td>Collins, Suzanne</td>\n",
       "      <td>792,187</td>\n",
       "      <td>Scholastic Ltd.</td>\n",
       "      <td>Young Adult Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Lost Boy,The:A Foster Child's Search for the L...</td>\n",
       "      <td>Pelzer, Dave</td>\n",
       "      <td>791,507</td>\n",
       "      <td>Orion</td>\n",
       "      <td>Biography: General</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Jamie's Ministry of Food:Anyone Can Learn to C...</td>\n",
       "      <td>Oliver, Jamie</td>\n",
       "      <td>791,095</td>\n",
       "      <td>Penguin</td>\n",
       "      <td>Food &amp; Drink: General</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Book Title       Author Name  \\\n",
       "0                                   Da Vinci Code,The        Brown, Dan   \n",
       "1                Harry Potter and the Deathly Hallows     Rowling, J.K.   \n",
       "2            Harry Potter and the Philosopher's Stone     Rowling, J.K.   \n",
       "3           Harry Potter and the Order of the Phoenix     Rowling, J.K.   \n",
       "4                                Fifty Shades of Grey      James, E. L.   \n",
       "..                                                ...               ...   \n",
       "95                                          Ghost,The    Harris, Robert   \n",
       "96                     Happy Days with the Naked Chef     Oliver, Jamie   \n",
       "97              Hunger Games,The:Hunger Games Trilogy  Collins, Suzanne   \n",
       "98  Lost Boy,The:A Foster Child's Search for the L...      Pelzer, Dave   \n",
       "99  Jamie's Ministry of Food:Anyone Can Learn to C...     Oliver, Jamie   \n",
       "\n",
       "   Volumes sold        Publisher                        Genre  \n",
       "0     5,094,805       Transworld  Crime, Thriller & Adventure  \n",
       "1     4,475,152       Bloomsbury           Children's Fiction  \n",
       "2     4,200,654       Bloomsbury           Children's Fiction  \n",
       "3     4,179,479       Bloomsbury           Children's Fiction  \n",
       "4     3,758,936     Random House              Romance & Sagas  \n",
       "..          ...              ...                          ...  \n",
       "95      807,311     Random House   General & Literary Fiction  \n",
       "96      794,201          Penguin        Food & Drink: General  \n",
       "97      792,187  Scholastic Ltd.          Young Adult Fiction  \n",
       "98      791,507            Orion           Biography: General  \n",
       "99      791,095          Penguin        Food & Drink: General  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating dataframe\n",
    "Top_Books=pd.DataFrame({\"Book Title\":Book,\n",
    "                \"Author Name\":Author,\n",
    "                'Volumes sold':Volumes_sold,\n",
    "                'Publisher':Publisher,\n",
    "                'Genre':Genre})\n",
    "print('\\033[1m'+'Best Selling Books of All Time List by The Guardian  :'+'\\033[0m')\n",
    "Top_Books"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be3956f",
   "metadata": {},
   "source": [
    "7. Scrape the details most watched tv series of all time from imdb.com.\n",
    "Url = https://www.imdb.com/list/ls095964455/ You have\n",
    "to find the following details:\n",
    "\n",
    "A) Name\n",
    "\n",
    "B) Year span\n",
    "\n",
    "C) Genre\n",
    "\n",
    "D) Run time\n",
    "\n",
    "E) Ratings\n",
    "F) Votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b1b28f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "from selenium.common.exceptions import ElementNotInteractableException\n",
    "import time\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "faa25b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting to webdriver\n",
    "driver=webdriver.Chrome()\n",
    "time.sleep(1)\n",
    "\n",
    "# Opening Wikipedia webpage\n",
    "url='https://www.imdb.com/list/ls095964455/'\n",
    "driver.get(url)\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e66f8066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Empty List\n",
    "Name =[]\n",
    "Year_Span=[]\n",
    "Genre =[]\n",
    "Run_Time =[]\n",
    "Ratings =[]\n",
    "Votes =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "918402a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping Name via Xpath\n",
    "try:\n",
    "    name=driver.find_elements(By.XPATH,'//h3[@class=\"lister-item-header\"]/a')\n",
    "    for i in name:\n",
    "        Name.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Name.append('NA')\n",
    "\n",
    "# Scraping Year span via Xpath\n",
    "try:\n",
    "    year=driver.find_elements(By.XPATH,\"//h3[@class='lister-item-header']/span[2]\")\n",
    "    for i in year:\n",
    "        Year_Span.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Year_Span.append('NA')\n",
    "    \n",
    "# Scraping Genre via Xpath\n",
    "try:\n",
    "    genre=driver.find_elements(By.XPATH,\"//span[@class='genre']\")\n",
    "    for i in genre:\n",
    "        Genre.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Genre.append('NA')\n",
    "    \n",
    "# Scraping RunTime via Xpath\n",
    "try:\n",
    "    runtime=driver.find_elements(By.XPATH,\"//span[@class='runtime']\")\n",
    "    for i in runtime:\n",
    "        Run_Time.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Run_Time.append('NA')\n",
    "\n",
    "# Scraping Ratings via Xpath\n",
    "try:\n",
    "    ratings=driver.find_elements(By.XPATH,\"//div[@class='ipl-rating-star small']/span[2]\")\n",
    "    for i in ratings:\n",
    "        Ratings.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Ratings.append('NA')\n",
    "\n",
    "# Scraping Votes via Xpath\n",
    "try:\n",
    "    votes=driver.find_elements(By.XPATH,\"//span[@name='nv']\")\n",
    "    for i in votes:\n",
    "        Votes.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Votes.append('NA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9dfc7656",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Year Span</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Run Time</th>\n",
       "      <th>Ratings</th>\n",
       "      <th>Votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Game of Thrones</td>\n",
       "      <td>(2011–2019)</td>\n",
       "      <td>Action, Adventure, Drama</td>\n",
       "      <td>57 min</td>\n",
       "      <td>9.2</td>\n",
       "      <td>2,214,059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Stranger Things</td>\n",
       "      <td>(2016–2025)</td>\n",
       "      <td>Drama, Fantasy, Horror</td>\n",
       "      <td>51 min</td>\n",
       "      <td>8.7</td>\n",
       "      <td>1,283,693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Walking Dead</td>\n",
       "      <td>(2010–2022)</td>\n",
       "      <td>Drama, Horror, Thriller</td>\n",
       "      <td>44 min</td>\n",
       "      <td>8.1</td>\n",
       "      <td>1,050,678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13 Reasons Why</td>\n",
       "      <td>(2017–2020)</td>\n",
       "      <td>Drama, Mystery, Thriller</td>\n",
       "      <td>60 min</td>\n",
       "      <td>7.5</td>\n",
       "      <td>308,686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The 100</td>\n",
       "      <td>(2014–2020)</td>\n",
       "      <td>Drama, Mystery, Sci-Fi</td>\n",
       "      <td>43 min</td>\n",
       "      <td>7.6</td>\n",
       "      <td>267,632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Reign</td>\n",
       "      <td>(2013–2017)</td>\n",
       "      <td>Drama</td>\n",
       "      <td>42 min</td>\n",
       "      <td>7.5</td>\n",
       "      <td>52,971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>A Series of Unfortunate Events</td>\n",
       "      <td>(2017–2019)</td>\n",
       "      <td>Adventure, Comedy, Drama</td>\n",
       "      <td>50 min</td>\n",
       "      <td>7.8</td>\n",
       "      <td>65,035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Criminal Minds</td>\n",
       "      <td>(2005– )</td>\n",
       "      <td>Crime, Drama, Mystery</td>\n",
       "      <td>42 min</td>\n",
       "      <td>8.1</td>\n",
       "      <td>211,973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Scream: The TV Series</td>\n",
       "      <td>(2015–2019)</td>\n",
       "      <td>Comedy, Crime, Drama</td>\n",
       "      <td>45 min</td>\n",
       "      <td>7</td>\n",
       "      <td>44,124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>The Haunting of Hill House</td>\n",
       "      <td>(2018)</td>\n",
       "      <td>Drama, Horror, Mystery</td>\n",
       "      <td>572 min</td>\n",
       "      <td>8.6</td>\n",
       "      <td>270,582</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Name    Year Span                     Genre  \\\n",
       "0                  Game of Thrones  (2011–2019)  Action, Adventure, Drama   \n",
       "1                  Stranger Things  (2016–2025)    Drama, Fantasy, Horror   \n",
       "2                 The Walking Dead  (2010–2022)   Drama, Horror, Thriller   \n",
       "3                   13 Reasons Why  (2017–2020)  Drama, Mystery, Thriller   \n",
       "4                          The 100  (2014–2020)    Drama, Mystery, Sci-Fi   \n",
       "..                             ...          ...                       ...   \n",
       "95                           Reign  (2013–2017)                     Drama   \n",
       "96  A Series of Unfortunate Events  (2017–2019)  Adventure, Comedy, Drama   \n",
       "97                  Criminal Minds     (2005– )     Crime, Drama, Mystery   \n",
       "98           Scream: The TV Series  (2015–2019)      Comedy, Crime, Drama   \n",
       "99      The Haunting of Hill House       (2018)    Drama, Horror, Mystery   \n",
       "\n",
       "   Run Time Ratings      Votes  \n",
       "0    57 min     9.2  2,214,059  \n",
       "1    51 min     8.7  1,283,693  \n",
       "2    44 min     8.1  1,050,678  \n",
       "3    60 min     7.5    308,686  \n",
       "4    43 min     7.6    267,632  \n",
       "..      ...     ...        ...  \n",
       "95   42 min     7.5     52,971  \n",
       "96   50 min     7.8     65,035  \n",
       "97   42 min     8.1    211,973  \n",
       "98   45 min       7     44,124  \n",
       "99  572 min     8.6    270,582  \n",
       "\n",
       "[100 rows x 6 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating dataframe\n",
    "IMDB_TV=pd.DataFrame({\"Name\":Name,\n",
    "                \"Year Span\":Year_Span,\n",
    "                \"Genre\":Genre,\n",
    "                \"Run Time\":Run_Time,\n",
    "                \"Ratings\":Ratings,\n",
    "                \"Votes\":Votes})\n",
    "\n",
    "IMDB_TV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2690d55",
   "metadata": {},
   "source": [
    "8. Details of Datasets from UCI machine learning repositories.\n",
    "Url = https://archive.ics.uci.edu/ You\n",
    "have to find the following details:\n",
    "    \n",
    "A) Dataset name\n",
    "\n",
    "B) Data type\n",
    "\n",
    "C) Task\n",
    "\n",
    "D) Attribute type\n",
    "\n",
    "E) No of instances\n",
    "\n",
    "F) No of attribute\n",
    "\n",
    "G) Year\n",
    "\n",
    " Note: - from the home page you have to go to the Show All Dataset page through code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "289bc44b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No link found to 'View All Data Sets' page on the homepage.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Send a GET request to the UCI Machine Learning Repository homepage\n",
    "url = \"https://archive.ics.uci.edu/\"\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Find the link to the \"View All Data Sets\" page\n",
    "    all_datasets_link = soup.find(\"a\", href=\"ml/datasets.php\")\n",
    "    if all_datasets_link:\n",
    "        all_datasets_url = url + all_datasets_link.get(\"href\")\n",
    "\n",
    "        # Send a GET request to the \"View All Data Sets\" page\n",
    "        all_datasets_response = requests.get(all_datasets_url)\n",
    "\n",
    "        if all_datasets_response.status_code == 200:\n",
    "            all_datasets_soup = BeautifulSoup(all_datasets_response.content, \"html.parser\")\n",
    "\n",
    "            # Find the table containing the dataset details\n",
    "            dataset_table = all_datasets_soup.find(\"table\", class_=\"table\")\n",
    "\n",
    "            # Initialize lists to store data\n",
    "            dataset_names = []\n",
    "            data_types = []\n",
    "            tasks = []\n",
    "            attribute_types = []\n",
    "            num_instances = []\n",
    "            num_attributes = []\n",
    "            years = []\n",
    "\n",
    "            # Extract data for each dataset\n",
    "            for row in dataset_table.find_all(\"tr\")[1:]:\n",
    "                columns = row.find_all(\"td\")\n",
    "                if len(columns) == 9:\n",
    "                    dataset_names.append(columns[0].get_text().strip())\n",
    "                    data_types.append(columns[1].get_text().strip())\n",
    "                    tasks.append(columns[2].get_text().strip())\n",
    "                    attribute_types.append(columns[3].get_text().strip())\n",
    "                    num_instances.append(columns[4].get_text().strip())\n",
    "                    num_attributes.append(columns[5].get_text().strip())\n",
    "                    years.append(columns[8].get_text().strip())\n",
    "\n",
    "            # Create a DataFrame from the collected data\n",
    "            data = {\n",
    "                'Dataset Name': dataset_names,\n",
    "                'Data Type': data_types,\n",
    "                'Task': tasks,\n",
    "                'Attribute Type': attribute_types,\n",
    "                'No of Instances': num_instances,\n",
    "                'No of Attributes': num_attributes,\n",
    "                'Year': years\n",
    "            }\n",
    "\n",
    "            df = pd.DataFrame(data)\n",
    "\n",
    "            # Print the DataFrame\n",
    "            print(df)\n",
    "\n",
    "        else:\n",
    "            print(\"Failed to retrieve the 'View All Data Sets' page. Status code:\", all_datasets_response.status_code)\n",
    "    else:\n",
    "        print(\"No link found to 'View All Data Sets' page on the homepage.\")\n",
    "else:\n",
    "    print(\"Failed to retrieve the UCI Machine Learning Repository homepage. Status code:\", response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19200f63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
